\documentclass{article}
%==============================================================================%
%	                          Packages                                     %
%==============================================================================%
% Packages
\usepackage[utf8]{inputenc}
\usepackage{graphicx}
\usepackage{amsmath}
\usepackage{amssymb}
\usepackage{braket}
\usepackage[margin=0.7in]{geometry}
\usepackage[version=4]{mhchem}
%==============================================================================%
%                           User-Defined Commands                              %
%==============================================================================%
% User-Defined Commands
\newcommand{\be}{\begin{equation}}
\newcommand{\ee}{\end{equation}}
\newcommand{\benum}{\begin{enumerate}}
\newcommand{\eenum}{\end{enumerate}}
\newcommand{\pd}{\partial}
\newcommand{\dg}{\dagger}
\newcommand{\sumzero}{\sum_{n=0}^\infty}
\newcommand{\sumone}{\sum_{n=1}^\infty}
%==============================================================================%
%                             Title Information                                %
%==============================================================================%
\title{Chem237: Lecture 8}
\date{4/10/18}
\author{Alan Robledo}
%==============================================================================%
%	Everyone Please Make Comments if Something Needs to be Reviewed        %
%                           Or just fix it yourself!                           %
%==============================================================================%
\begin{document}
\maketitle

\subsection*{Approximate Methods}
So far, we have discussed different ways to evaluate integrals of interest to obtain exact answers.
It is often times enough for us to know an approximation to the value of an integral and we will discuss a few methods that go about this by looking at some special functions as examples.
\subsubsection*{Asymptotic Series}
At first glance, you'd think that the error function is just the integral of a gaussian
\be
  \text{erf(x)} = \frac{2}{\sqrt{\pi}} \int_0^x e^{-t^2} dt = \frac{1}{\sqrt{\pi}} \int_{-x}^x e^{-t^2} dt
\ee
and you'd be correct. To be specific, the error function is the integral of a normalized Gaussian with the normalization part coming from the $\frac{1}{\sqrt{\pi}}$.
The error function comes up in probability theory as this function is used to show the probability of something lying between -x and x.
Finding a good approximation of the integral can be done by taylor expanding the integrand about a center that we will take to be zero, and integrating term by term.
\be
  \begin{split}
    \text{erf(x)} &= \frac{2}{\sqrt{\pi}} \int_0^x e^{-t^2} dt \\
    &= \frac{2}{\sqrt{\pi}} \int_0^x (1 - t^2 + \frac{t^4}{2!} - \frac{t^6}{3!} + \hdots) dt \\
    &= \frac{2}{\sqrt{\pi}} (x - \frac{x^3}{3} + \frac{x^5}{5 \cdot 2!} - \frac{x^7}{7 \cdot 3!} + \hdots)
  \end{split}
\ee
By the look of it, we can say that the series converges for any x because the denominator tends to infinity much faster than the numerator ever could.
However, the convergence of the series is small for large x.
As a matter of fact, for large x, as x $\rightarrow \infty$, erf(x) $\rightarrow$ 1.
So suppose we wanted to know what erf(x) is for large x.
Using the fact that erf(x) $\rightarrow$ 1 as x $\rightarrow \infty$, we could write
\be
1 = \frac{2}{\sqrt{\pi}} \int_0^{\infty} e^{-t^2} dt
\ee
and if we break up the integrals
\be
1 = \frac{2}{\sqrt{\pi}} \int_0^x e^{-t^2} dt + \frac{2}{\sqrt{\pi}} \int_x^{\infty} e^{-t^2} dt
\ee
and rearrange terms, we get
\be \label{eq:erfc}
1 - \frac{2}{\sqrt{\pi}} \int_0^x e^{-t^2} dt = 1 - \text{erf(x)} = \frac{2}{\sqrt{\pi}} \int_x^{\infty} e^{-t^2} dt = \text{erfc(x)}
\ee
where we have an integral from large x to infinity, which happens to be called the complementary error function or erfc(x).
We can try to find an expression for the complementary error function by considering using integration by parts.
Now, you may look at the integrand and think to yourself that there are no two functions that you can use for integration by parts.
But a neat little trick that is often used is to consider multiplying the integrand by the one function.
\be
  \int_x^{\infty} e^{-t^2} dt = \int_x^{\infty} e^{-t^2} \cdot 1 dt = \int_x^{\infty} e^{-t^2} \cdot \frac{-2t}{-2t} dt
\ee
and if we consider,
\be
  \begin{split}
    U &= - \frac{1}{2t} \qquad dV = - 2t e^{-t^2} dt \\
    dU &= \frac{1}{2t^2} dt \qquad V = e^{-t^2}
  \end{split}
\ee
our integral becomes
\be
  \begin{split}
    \int_x^{\infty} e^{-t^2} dt &= \frac{e^{-t^2}}{2t} \Big|_x^{\infty} - \int_x^{\infty} \frac{e^{-t^2}}{2t^2} dt \\
    &= \frac{e^{-x^2}}{2x} - \int_x^{\infty} \frac{e^{-t^2}}{2t^2} dt
  \end{split}
\ee
We can integrate by parts again by using the same trick of multiplying the integrand by the one function
\be
  \int_x^{\infty} e^{-t^2} dt = \frac{e^{-x^2}}{2x} - \int_x^{\infty} \frac{e^{-t^2}}{2t^2} \cdot \frac{-2t}{-2t} dt
\ee
and considering
\be
  \begin{split}
    U &= - \frac{1}{4t^3} \qquad dV = - 2t e^{-t^2} dt \\
    dU &= \frac{3}{4t^4} dt \qquad V = e^{-t^2}
  \end{split}
\ee
to get
\be
  \begin{split}
    \int_x^{\infty} e^{-t^2} dt &= \frac{e^{-x^2}}{2x} - \int_x^{\infty} \frac{e^{-t^2}}{2t^2} \\
    &= \frac{e^{-x^2}}{2x} + \frac{e^{-t^2}}{4t^3} \Big|_x^{\infty} + \int_x^{\infty} \frac{3e^{-t^2}}{4t^4} dt \\
    &= \frac{e^{-x^2}}{2x} - \frac{e^{-x^2}}{4x^3} + \int_x^{\infty} \frac{3e^{-t^2}}{4t^4} dt
  \end{split}
\ee
You can see that we can use this trick each time we integrate by parts and if we integrated an infinite number of times, we'd have an exact expression.
If we go back to the equation \ref{eq:erfc} and solve for erf(x), we can see that the result after n integrations by parts of the complementary error function gives
\be
  \begin{split}
    \text{erf(x)} = 1 - \frac{2}{\sqrt{\pi}} \int_x^{\infty} e^{-t^2} dt = 1 - \frac{2}{\sqrt{\pi}} e^{-x^2} \Big[ \frac{1}{2x} - \frac{1}{2^2 x^3} & + \frac{1 \cdot 3}{2^3 x^5} - \frac{1 \cdot 3 \cdot 5}{2^4 x^7} + \hdots + (-1)^{n-1} \frac{1 \cdot 3 \cdot 5 \cdots (2n-3)}{2^n x^{2n-1}} \Big] \\
    & + (-1)^n \frac{1 \cdot 3 \cdot 5 \cdots (2n-1)}{2^n} \frac{2}{\sqrt{\pi}} \int_x^{\infty} \frac{e^{-t^2}}{t^{2n}} dt
  \end{split}
\ee
where the last term (the term after the brackets, including the integral) is the "remainder" that makes the expression exact.
If we only pay attention to the terms inside the brackets, we can see that these terms form the beginning of divergent series.
Since the numerator grows as (2n+1)!, there is no value of x that would make the denominator grow larger in size than the numerator so if we were to form an infinite series, the series would diverge to $\infty$.
It is because of this fact that physicists would call the series in brackets an \textbf{asymptotic series} if the summation of the terms continued to infinity.
To be formal, if we have a function whose series expansion is
\be
S(x) = c_0 + \frac{c_1}{x} + \frac{c_2}{x} + \hdots
\ee
where the c$_n$'s are just constants, we can say that the partial sum $\sum\limits_{i=0}^n c_i / x^i$ is an asymptotic series expansion of f(x) if the following holds
\be
  \lim_{x \to \infty} x^n \Big[ f(x) - \sum_{i=0}^n \frac{c_i}{x^i} \Big] = 0
\ee
Therefore, for some value n and for arbitrarily large x, the series $S_n(x)$ gives a good approximation to f(x).
If, however, we have a function whose series expansion is
\be
  S(x) = c_0 + c_1x + c_2x^2 + \hdots
\ee
where the c$_n$'s are again just constants, we can say that the partial sum  $\sum\limits_{i=0}^n c_i x^i$ is an asymptotic series expansion of f(x) if the following holds
\be
  \lim_{x \to \infty} \frac{1}{x^n} \Big[ f(x) - \sum_{i=0}^n c_i x^i \Big] = 0
\ee
Therefore, for some value n and for arbitrarily small x, the series $S_n(x)$ gives a good approximation to f(x).

Mathematicians define an asymptotic series expansion as a series whose partial sums give a good approximation to a function of a variable x for arbitrarily small or large values of x.
This definition makes it sound like a convergent Taylor series falls under the category of an asymptotic expansion.
Because of this, it is common in Physics for the term 'asymptotic expansion' to imply a divergent series.
To be mathematically correct, an asymptotic series can be divergent or convergent but, in most cases of interest, asymptotic series never converge.
So it is useful to consider the physicist's perspective when distinguishing between a convergent series and an asympotitc series.
We can go back to the error function erf(x) to show how an asymtptic series can be different from a convergent series.
\be
  \text{erf(x)} = \frac{2}{\sqrt{\pi}} \int_0^x e^{-t^2} dt
\ee
Just as we did in equation 2, if we taylor expand the integrand about $t=0$, and integrate the series term by term, we get the following convergent series,
\be
  \text{erf(x)} = \frac{2}{\sqrt{\pi}} \Big( x - \frac{1}{3}x^3 + \hdots + \frac{(-1)^n}{(2n+1)n!} x^{2n+1} + \hdots \Big)
\ee
which is convergent for all values of x.
And if we recall from earlier, we obtained an asymptotic series of the erf(x)
\be
  \text{erf(x)} \approx 1 - \frac{2}{\sqrt{\pi}} e^{-x^2} \Big[ \frac{1}{2x} - \frac{1}{2^2 x^3} + \frac{1 \cdot 3}{2^3 x^5} - \frac{1 \cdot 3 \cdot 5}{2^4 x^7} + \hdots + (-1)^{n-1} \frac{1 \cdot 3 \cdot 5 \cdots (2n-3)}{2^n x^{2n-1}} \Big] + \text{remainder}
\ee
Now, when $x=3$, we need the first 30 terms of the taylor series to approximate erf(3) to an accuracy of $10^{-5}$, whereas we only need about the first two terms of the asymptotic series (excluding the remainder of course) to obtain the same approximation.
This should not be of any surprise to us because I mentioned earlier in the lecture that the convergence of the Taylor series is slow for (arbitrarily) large x.
If we were to make a plot of the remainder as a function of the number of terms we decide to keep in our series approximation for fixed x
%==============================================================================%
%  Add plot of remainder as a function of M in notes.
%==============================================================================%
we would see that as we keep adding more terms, our approximation gets better and better up to a certain limit, which defines the asymptoticity of the function, and then our approximation becomes worse and worse.
This should make sense because we are trying to use a divergent series to define the value of an integral.

Note, if we use the physics definition of asymptotic series then we have to be clear in saying that not all functions have an asymptotic expansion.
If a function does have such an expansion then that expansion is unique.
However, one can get different asympotitc series expansions for the same function when we consider x to be a complex number z and the convergence of a series to be defined by the radius of a disk on the complex plane.
This is known in complex analysis as Stokes phenomenon but we will not dive into the topic.

\subsubsection*{Saddle Point Method}
Consider the integral
\be
  \int_0^{\infty} F(x) dx = \int_0^{\infty} e^{f(x)} dx
\ee
where the integral of F(x) is dominated by a narrow region in x.
A example plot of F(x) as a function of x would be
%==============================================================================%
%  Add plot of remainder as a function of M in notes.
%==============================================================================%
If we perform a Taylor expansion of f(x) and decide to expand around the point x$_o$ we get
\be
  F(x) \approx \text{exp} \Big[ f(x_o) + (x - x_o)f'(x_o) + \frac{(x - x_o)^2}{2}f''(x_o) \Big]
\ee
If we let x$_o$ to be the maximum of f(x), we can identify that $f'(x_o) = 0$ so our integral becomes
\be
  \begin{split}
    \int_0^{\infty} F(x) dx &\approx \int_0^{\infty} \text{exp} \big[ f(x_o) + \frac{(x - x_o)^2}{2}f''(x_o) \Big] dx \\
    &= \text{exp}[f(x_o)] \int_0^{\infty} \text{exp} \Big[ \frac{(x - x_o)^2}{2}f''(x_o) \Big] dx
  \end{split}
\ee
which is just the integral of a gaussian exp $[A(x-x_o)^2]$ where $A = \frac{f''(x_o)}{2}$.
Using a u-substitution $u = x - x_o$ and $du = dx$, you should be able to get
\be
  \begin{split}
      \int_0^{\infty} F(x) dx &\approx \text{exp}[f(x_o)] \int_0^{\infty} \text{exp} \Big[ \frac{(x - x_o)^2}{2}f''(x_o) \Big] dx \\
      &= \text{exp}[f(x_o)] \Big( \frac{-2 \pi}{f''(x_o)} \Big) ^{\frac{1}{2}}
  \end{split}
\ee
where $f''(x_o) < 0$ to keep from getting imaginary results.
As an example, we will consider another special function: the Gamma function.
\be
  \Gamma(x+1) = \int_0^{\infty} t^{x} e^{-t} dt
\ee
We can turn the integrand into an exponential with the following transformation
\be
  t^{x} e^{-t} = e^{\ln(t^x)} e^{-t} = e^{x \ln(t)} e^{-t} = e^{x \ln(t) - t}
\ee
and use the saddle-point approximation to approximate the value of the integral.
We will consider $f(t) = x \ln(t) - t$ and Taylor expand f(t) up to second order.
So we need to compute some derivatives and evaluate them at the maximum $t_o$.
\be
  \begin{split}
    f(t) \Big|_{t=t_o} &= x \ln(t_o) - t_o \\
    f'(t) \Big|_{t=t_o} &= \frac{x}{t_o} - 1 \\
    f''(t) \Big|_{t=t_o} &= -\frac{x}{t_o^2}
  \end{split}
\ee
Since we know that the value of the first derivative at the maximum is zero, we can use the second equation to solve for zero and we get the relationship $t_o = x$.
Replacing all the $t_o$'s with x, we get the following expansion
\be
  f(t) \approx x \ln(x) - x - \frac{1}{2!} \cdot \frac{1}{x} (t - x)^2
\ee
So our integral becomes
\be
  \begin{split}
    \Gamma(x+1) &= \int_0^{\infty} t^{x} e^{-t} dt \\
    &= \int_0^{\infty} e^{x \ln(x) - x - \frac{1}{2x} (t - x)^2} \\
    &= \int_0^{\infty} e^{x \ln(x) - x} e^{- \frac{1}{2x} (t - x)^2} dt \\
    &= e^{\ln(x^x) - x} \int_0^{\infty} e^{- \frac{1}{2x} (t - x)^2} dt \\
    &= x^x e^{-x} \int_0^{\infty} e^{- \frac{1}{2x} (t - x)^2} dt
  \end{split}
\ee
which is of course another integral of a gaussian.
Using a u-substitution $u = t - x$ and $du = dx$ (remember, x is considered a constant in this integral) we have
\be
  \begin{split}
    \Gamma(x+1) &\approx x^x e^{-x} \int_0^{\infty} e^{- \frac{1}{2x} (t - x)^2} dt \\
    &= x^x e^{-x} \int_0^{\infty} e^{- \frac{1}{2x} u^2} du \\
    &= \sqrt{2 \pi x} x^x e^{-x} \\
    &= \sqrt{2 \pi x} \Big( \frac{x}{e} \Big)^x
  \end{split}
\ee
which is actually the first half of proving stirling's approximation!
The second half of the proof involves showing that $\Gamma(x+1) = x!$, which can be done by using multiple integration by parts, which shouldn't be too bad.
Taking $t^x = U$ and $e^{-t} = dV$, we get
\be
  \begin{split}
    \Gamma(x+1) &= \int_0^{\infty} t^{x} e^{-t} dt \\
    &= - t^x e^{-t} \Big|_0^{\infty} + x \int_0^{\infty} t^{x-1} e^{-t} dt \\
  \end{split}
\ee
and since $- t^x e^{-t} \Big|_0^{\infty}$ goes to zero, we're just left with $\Gamma(x)$.
So we can integrate by parts a few more times until we see a pattern
\be
  \begin{split}
    \Gamma(x+1) &= x \Gamma(x) \\
    &= x \int_0^{\infty} t^{x-1} e^{-t} dt \\
    &= x \Big( -t^{x-1}e^{-t} \Big|_0^{\infty} + (x-1) \int_0^{\infty} t^{x-2} e^{-t} dt \Big) \\
    &= x(x-1) \int_0^{\infty} t^{x-2} e^{-t} dt \\
    &= x(x-1) \Gamma(x) \\
    &= \hdots \\
    &= x(x-1)\cdot2\cdot1 \int_0^{\infty} e^{-t} dt \\
    &= - (x!) e^{-t} \Big|_0^{\infty} \\
    &= -x! (0 - 1)\\
    &= x!
  \end{split}
\ee
And from our result from earlier, we have that for any integer x = n
\be
  \Gamma(n+1) = n! \approx \sqrt{2 \pi n} \Big( \frac{n}{e} \Big)^n
\ee
which is Stirling's approximation!
%==============================================================================%
% Relate it to Stat mech if you want.
%==============================================================================%
And with that, we are done with chapter 3 of the Math Methods book.
\subsection*{Integral Transforms}
Chapter 4 can be thought of as a continuation of chapter 3 in that we are still going to be looking at integrals and finding ways to solve them.
But, similar to contour integration, we are going to be focusing on how to map our function of interest from one domain onto another domain in order to make our integrals much easier to solve.
Then we would map our solutions back to the original domain to get the appropriate result.
This is done by considering different types of integral transforms that are used depending on the behavior of our function/integrand.
Some include: Fourier Transform, Laplace Transform, Hilbert Transform, etc, but we will only be discussing the first two as they are very commonly used in Physics.
\subsubsection*{Fourier Series}
The topic of Fourier Series/Transform is one that is discussed often in physics classes and some Physical Chemistry classes.
However, the topic is usually introduced through example and you are only required to know how to obtain the series and/or perform the transform and then understand how it can be applied.
Our Math Methods book introduces the topic the same way.
For those of you who have been taught the subject this way or for those who have only heard of it, you might be aware that there are different conventions associated with the Series/Transform.
But, despite the existence of different conventions, each give the same results to the same problems.

The concept of a Fourier Series is made more intuitive when understood in the context of \textbf{Linear Vector Spaces}.
Linear Vector Spaces are discussed heavily in Linear Algebra courses and are thought about constantly in Quantum Mechanics.
We will be covering Linear Algebra later in the course, but if you have not been exposed to Linear Algebra you can refer to Lecture 12 where we discuss it in full detail.
Or you can try following along until you don't understand something and keep going back and forth with both lectures and whatever other material you may be using.
We will start with basic general definitions that are discussed in any textbook but we will soon make the connection to vector spaces to get a better understanding of where things come from.

When we have a function $f(\theta)$ that is defined periodically for $0 \leq \theta \leq 2 \pi$, we can create an expansion that is of the form
\be
  f(\theta) = \sum_{n = 0}^{\infty} A_n \cos(n \theta) + B_n \sin(n \theta)
\ee
This is considered the general form of a fourier series and the coefficients can be found analytically.
When we say that a function is periodic, we mean that the function repeats in intervals $- 2\pi \leq \theta \leq 0$, $- \pi \leq \theta \leq \pi$, $2 \pi \leq \theta \leq 4 \pi $, etc.
It is only required that the interval is of length $2\pi$.
If we extend or contract the length of the interval, we must change the form of the sine and cosine accordingly.
For the time being, we will stick to the interval $0 \leq \theta \leq 2 \pi$ and discuss changing the length of the interval later.

One of the nice properties of sine and cosine functions is that they are orthogonal on given intervals.
What it means for our trig functions to be orthogonal is that if we have two functions $\sin(nx)$ and $\cos(mx)$ and integrate their product over the interval $0 \leq x \leq 2 \pi$ we get
\be \label{eq:sin_cos}
  \int_0^{2\pi} \sin(nx)\cos(mx) dx = 0
\ee
for any n and m.
And if take the the product of two sine functions $\sin(nx)$ and $\sin(mx)$, then
\be \label{eq:sin_sin_orth}
  \begin{split}
    \int_0^{2\pi} \sin(nx)\sin(mx) dx &= 0 \quad \text{if} \quad n \neq m \\
    \int_0^{2\pi} \sin(nx)\sin(mx) dx &= \pi \quad \text{if} \quad n = m
  \end{split}
\ee
Likewise, if we have two cosine functions $\cos(nx)$ and $\cos(mx)$, then
\be \label{eq:cos_cos_orth}
  \begin{split}
    \int_0^{2\pi} \cos(nx)\cos(mx) dx &= 0 \quad \text{if} \quad n \neq m \\
    \int_0^{2\pi} \cos(nx)\cos(mx) dx &= \pi \quad \text{if} \quad n = m
  \end{split}
\ee
where n and m are integers $n,m = 1, 2, 3, \hdots$.
If we wanted to normalize our trig functions, then we must consider every sine and cosine having a normalization constant $a_n$ and $b_n$ such that the product of sines is equal to one
\be \label{eq:sin_sin_norm_norm}
  \int_0^{2\pi} a_n^2 \sin^2(nx) = a_n^2\pi = 1 \Rightarrow a_n = \frac{1}{\sqrt{\pi}}
\ee
and the product of cosines is also equal to one
\be \label{eq:cos_cos_norm_norm}
  \int_0^{2\pi} a_n^2 \cos^2(nx) = b_n^2\pi = 1 \Rightarrow b_n = \frac{1}{\sqrt{\pi}}
\ee
So our functions are now orthonormal (orthogonal and normalized).
Therefore, if we rewrite equations \ref{eq:sin_sin_orth} and \ref{eq:cos_cos_orth} we get
\be \label{eq:sin_sin_orthonorm}
  \int_0^{2\pi} \sin(nx)\sin(mx) dx = \delta_{nm}
\ee
\be \label{eq:cos_cos_orthonorm}
  \int_0^{2\pi} \cos(nx)\cos(mx) dx = \delta_{nm}
\ee
where $\delta_{nm}$ is the kronecker delta and it is shorthand notation for
\be
\delta_{nm} =
  \left\{
    \begin{array}{ll}
      0 & \quad \text{if n} \neq \text{m} \\
      1 & \quad \text{if n = m}
    \end{array}
  \right.
\ee
Whether your decide to normalize your sines and cosines is one convention while not normalizing your sines and cosines is another convention.
We will use the convention of using normalized sines and cosines so that these integrals either evaluate to 0 or 1 for simplicity.

To introduce the concept of a linear vector space, remember that whenever we have a function, we can almost always represent it in a Taylor series expansion, giving us
\be
  f(x) = \sum_{n=0}^{\infty} h_n x^n
\ee
and (assuming our function can be Taylor expanded) we can think of the function as having a basis of polynomials of the form $x^n$ with weightings $h_n$ that are of the form
\be
  h_n = \frac{f^{(n)}(x_o)}{n!}
\ee
From lecture 3, we showed that sine and cosine both have Taylor expansions of the form
\be
  \begin{split}
    \sin(x) &= 0 + x + 0\frac{x^2}{2!} - \frac{x^3}{3!} + \hdots = x - \frac{x^3}{3!} + \frac{x^5}{5!} - \hdots \\
    \cos(x) &= 1 + 0x - \frac{x^2}{2!} + 0 \frac{x^3}{3!} + \hdots = 1 - \frac{x^2}{2!} + \frac{x^4}{4!} - \hdots
  \end{split}
\ee
And we can see that $\sin(x)$ and $\cos(x)$ are functions of x with odd and even powers.
But when $\sin(x)$ and $\cos(x)$ are added together, we will always have an infinite degree polynomial in x.
\be
  \cos(x) + \sin(x) = (1 - \frac{x^2}{2!} + \frac{x^4}{4!} - \hdots) + (x - \frac{x^3}{3!} + \frac{x^5}{5!} - \hdots) = 1 + x - \frac{x^2}{2!} - \frac{x^3}{3!} + \frac{x^4}{4!} + \hdots
\ee

If we consider a set of all polynomials $x^n$ together with the addition of polynomials and the multiplication of a polynomial by a number, we have an infinite dimensional linear vector space where our polynomials themselves are vectors.
Likewise, we can think of a set of all linear combinations of $\sin(nx)$ and $\cos(nx)$ as a vector space, where $\sin(nx)$ and $\cos(nx)$ are orthogonal basis vectors.
In Physics, this comes up all the time when we discuss Hermitian operators because it turns out that Hermitian operators often have eigenvectors of the form $\sin(nx)$ and $\cos(mx)$.
If you're thinking ahead, we can also have an complex exponential $e^{ix}$ or $e^{-ix}$ as a basis vector as well but we will consider this later in lecture.

Going back to our fourier series,
\be
  f(\theta) = \sum_{n = 0}^{\infty} A_n \cos(n \theta) + B_n \sin(n \theta)
\ee
now that we can think of having an infinite vector space where our functions are vectors, we can think of this series as a sum of orthonormal basis vectors in the form of sine and cosine with weightings $A_n$ and $B_n$.
If we use dirac notation, we can make our series look like
\be
  \ket{f(\theta)} = \sum_{n=0}^{\infty} A_n \ket{c_n} + B_n \ket{s_n}
\ee
where the $\ket{c_n}$'s are our cosines with integer n and the $\ket{s_n}$'s are our sines with integer n.

Since we now have a vector space, we want to be able to define an inner product (you may remember it as the dot product) on our vector space.
The reason why we want to define an inner product is because we want to be able to define an orthogonal projection of one vector onto another.
When we project a vector $\vec{u}$ onto another vector $\vec{v}$, we consider this as taking the dot product
\be
  \vec{u} \cdot \vec{v} = u_1v_1 + u_2v_2 + \hdots
\ee
and in dirac notation this looks like
\be
  \vec{u} \cdot \vec{v} = \braket{u|v}
\ee
And since our functions can be thought of as vectors, we can think of performing an inner product with our functions!
But if you're confused about how to do that, it turns out that taking an integral is exactly the same thing as the inner product!

When we have two vectors, the dot product is defined as multiplying a component from one vector and a component from another vector and adding up all the multiplications.
When we have two functions f and g, the integral of their product is defined as multiplying a function evaluated at one point and another function evaluated at a point, with each product having a weight dx, and adding up all the multiplications.
\be
  \int_a^b f(x)g(x) = f(a)g(a)dx + f(a + dx)g(a + dx)dx + \hdots
\ee
So our orthogonal projection is the same thing as taking the integral of the product of our orthogonal functions!
\be
  \braket{f|g} = \int_a^b f^*(x)g(x) dx
\ee
Notice that using dirac notation means that our function in the ket needs to be in the form of its complex conjugate when evaluating the integral.
If our sines and cosines are real functions however, then the complex conjugate of a real function is simply itself.

Now that we know we can take dot products of our series with other vectors, we can use our vectors' orthogonality properties to find an expression for the coefficients $A_n$ and $B_n$.
If we project the cosine vector $\bra{c_n}$ onto our $\ket{f(\theta)}$ vector (remember, projection means dot product), we get, in dirac notation,
\be
  \braket{c_n|f(\theta)} = \sum_{n=0}^{\infty} A_n \braket{c_n|c_n} + B_n \braket{c_n|s_n}
\ee
and since we know from equation \ref{eq:sin_cos} that the integral of sine and cosine is always zero, then the dot product $\braket{c_n|s_n}$ is always zero.
And since we know from equation \ref{eq:cos_cos_orthonorm} that $\braket{c_n|c_n}$ is 1 when $n = m$ and 0 when $n \neq m$ (don't forget about the normalization constants), then all the terms in the sum but one will cancel out and all that we're left with is
\be
  A_n = \braket{c_n|f(\theta)} = \frac{1}{\sqrt{\pi}} \int_0^{2\pi} \cos(n \theta) f(\theta) d\theta
\ee
If we project the sine vector $\bra{s_n}$ onto our $\ket{f(\theta)}$ vector, we get, in dirac notation
\be
  \braket{s_n|f(\theta)} = \sum_{n=0}^{\infty} A_n \braket{s_n|c_n} + B_n \braket{s_n|s_n}
\ee
And using the same logic as before, equation \ref{eq:sin_cos} says that $\braket{s_n|c_n}$ is zero for any n and m, and equation \ref{eq:sin_sin_orthonorm} says $\braket{s_n|s_n}$ will only be 1 when $n = m$, so we're left with
\be
  B_n = \braket{s_n|f(\theta)} = \frac{1}{\sqrt{\pi}} \int_0^{2\pi} \sin(n \theta) f(\theta) d\theta
\ee
If you check what the Math methods book has for the coefficients $A_n$ and $B_n$, you'll notice that there is a factor of $\frac{1}{\pi}$ in front of the integrals, whereas we have $\frac{1}{\sqrt{\pi}}$.
The reason for this is because (and some books do not mention this explicitly like ours) the book is choosing the convention where the basis functions are not normalized, whereas we have chosen to use the convention where the basis functions are normalized.
If you're starting to think that our convention is wrong, watch what happens when we explicitly write the fourier series with the coefficients written in their integral form using our convention.
\be \label{eq:fourier_check}
  \begin{split}
    f(\theta) &= \sum_{n = 0}^{\infty} \Big( \frac{1}{\sqrt{\pi}} \int_0^{2\pi} \cos(n \theta) f(\theta) d\theta \Big) \frac{1}{\sqrt{\pi}} \cos(n \theta) + \Big( \frac{1}{\sqrt{\pi}} \int_0^{2\pi} \sin(n \theta) f(\theta) d\theta \Big) \sin(n \theta) \\
    &= \sum_{n = 0}^{\infty} \Big( \frac{1}{\pi} \int_0^{2\pi} \cos(n \theta) f(\theta) d\theta \Big) \cos(n \theta) + \Big( \frac{1}{\pi} \int_0^{2\pi} \sin(n \theta) f(\theta) d\theta \Big) \sin(n \theta)
  \end{split}
\ee
The Fourier series coefficients end up becoming exactly as the book has them written.
So both conventions give the same result.
An issue that occurs in scientific computing is that some math software packages will program Fourier series such that there are no $\sqrt{\pi}$'s or $\pi$'s at all and you are free to add them wherever you like.
This becomes a third convention that simply does not make any sense mathematically and can cause confusion if you come across these packages.
But, now that we know Fourier series are just vectors in a linear vector space with sine and cosine as orthonormal basis vectors, we can use this knowledge as a check to make sure that our math always adds up correctly when we create the series from periodic functions.

This whole time, we've defined our Fourier series in an interval of length $2 \pi$ and we would now like to define a Fourier series that is periodic in an interval of any length L.
So if we wanted our Fourier series to go from representing functions that are periodic with some period $2\pi$ in the variable $\theta$ to representing functions that are periodic with some period L in the variable x, we let
\be
  x = \frac{\theta L}{2 \pi}
\ee
where $\frac{L}{2 \pi}$ can be thought of as a contraction/expansion ratio that is multiplied by our length $\theta$ to get our new length x.
If we rearrange the equation above we get
\be
 \theta = \frac{2 \pi x}{L}
\ee
and our Fourier series is now
\be
 f(\theta) = \sum_{n = 0}^{\infty} A_n \cos \Big( \frac{2 \pi n x}{L} \Big) + B_n \sin \Big( \frac{2 \pi n x}{L} \Big)
\ee
As for our coefficients, we don't need to change the $\frac{1}{\sqrt{\pi}}$ because that comes naturally from our normalized basis.
But we need to make the substitution
\be
  \theta = \frac{2 \pi x}{L} \qquad d\theta = \frac{2 \pi}{L} dx
\ee
So our $A_n$ becomes
\be
  \begin{split}
    A_n &= \braket{c_n|f(x)} \\
    &= \frac{1}{\sqrt{\pi}} \frac{2 \pi}{L} \int_0^{L} \cos \Big( \frac{2 n \pi x}{L} \Big) f(x) dx \\
    &= \frac{2 \sqrt{\pi}}{L} \int_0^{L} \cos \Big( \frac{2 n \pi x}{L} \Big) f(x) dx \\
  \end{split}
\ee
and our $B_n$ becomes
\be
  \begin{split}
    B_n &= \braket{s_n|f(x)} \\
    &= \frac{2 \sqrt{\pi}}{L} \int_0^{L} \sin \Big( \frac{2 n \pi x}{L} \Big) f(x) dx \\
  \end{split}
\ee
which is now defined on an interval of length L such as $0 \leq x \leq L$, $-L \leq x \leq 0$, etc.
Again, the book defines the coefficients with a $\frac{2}{L}$ but I can assure you that if you plug in our coefficients like I did in equation \ref{eq:fourier_check}, both conventions will yield the same series.

Now that we have a full understanding of where a Fourier series comes from, we will see how much easier it will be to define Fourier Transforms using our convention.
But for now, we will consider some examples that require us to create a Fourier Series to see where it can be applied.
However, we basically covered how to create a Fourier series if we are given any periodic function $f(\theta)$ or $f(x)$.
Therefore, we will consider some special cases where our given function is a periodic even or odd function.

If we have a periodic even function, we say that $f(\theta) = f(- \theta)$.
And if we wanted to express this function in terms of sine and cosine, since sine is an odd function (and this can be seen by seeing a plot of sine or by noticing that a Taylor expansion only gives polynomials with odd powers of x), our Fourier series will not have any sine contribution.
This means that we can skip having to evaluate the $B_n$ coefficients because $B_n$'s dictate the amount of sine contribution, which we concluded is always zero.
And all we need to evaluate are the $A_n$'s and we would be done.

Likewise, if we have a periodic odd function, we can say that $f(\theta) = - f(\theta)$.
And expressing the function in a Fourier series would only require us to evaluate the $B_n$ coefficients because there will be no cosine contribution since cosine is an even function.

Consider the following plot of $f(\theta)$
%==============================================================================%
%  Need the plot of the square function
%==============================================================================%
where the function can be defined as a piecewise function.
\be
  f(x) =
  \left\{
    \begin{array}{ll}
      +1 & \quad 0 < \theta < \pi \\
      -1 & \quad \pi < \theta < 2 \pi
    \end{array}
  \right.
\ee
If we make a plot of the piecewise function from $- \pi$ to $\pi$ with a plot of $\sin(x)$ superimposed on top of it,
%==============================================================================%
%  Need plot of pieceiwse function with sin(x) on top of it.
%==============================================================================%
we can see that this function is an odd function because $f(\theta) = - f(\theta)$, so we will only need $B_n$'s when creating a Fourier series representation.
Since $\sin(n \theta) = 0$ when $n = 0$, we don't have to consider the $B_0$ term and simply define the integral for $n = 1, 2, 3, \hdots$.
We then have
\be
  B_n = \braket{s_n|f(\theta)} = \frac{1}{\sqrt{\pi}} \int_0^{2 \pi} sin(n \theta) f(\theta) d\theta
\ee
where our upper and lower limits came from noticing that our function is periodic from 0 to $2 \pi$.
Since our function has different forms in the intervals $0 < \theta < \pi$ and $\pi < \theta < 2 \pi$, we could split up our integral into two integrals and evaluate
\be
  \begin{split}
    B_n &= \braket{s_n|f(\theta)} \\
    &= \frac{1}{\sqrt{\pi}} \int_0^{2 \pi} sin(n \theta) f(\theta) d\theta \\
    &= \frac{1}{\sqrt{\pi}} \int_0^{\pi} sin(n \theta) \cdot 1 d\theta + \frac{1}{\sqrt{\pi}} \int_{\pi}^{2 \pi} sin(n \theta) \cdot -1 d\theta \\
    &= \frac{1}{\sqrt{\pi}} \Big( - \frac{\cos(n \theta)}{n} \Big) \Big|_0^{\pi} + \frac{1}{\sqrt{\pi}} \Big( \frac{\cos(n \theta)}{n} \Big) \Big|_{\pi}^{2 \pi} \\
    &= \Big( - \frac{\cos(n \pi)}{n \sqrt{\pi}} + \frac{1}{n \sqrt{\pi}} \Big) + \Big( \frac{\cos(2 n \pi)}{n \sqrt{\pi}} - \frac{\cos(n \pi)}{n \sqrt{\pi}} \Big)
  \end{split}
\ee
Since $n = 1, 2, 3, \hdots$, we can say that $\cos(2 n \pi)$ will always be equal to 1, because any value of n will give this result.
\be
  \begin{split}
    B_n &= \frac{1}{\sqrt{\pi}} \int_0^{2 \pi} sin(n \theta) f(\theta) d\theta \\
    &= \Big( - \frac{\cos(n \pi)}{n \sqrt{\pi}} + \frac{1}{n \sqrt{\pi}} \Big) + \Big( \frac{1}{n \sqrt{\pi}} - \frac{\cos(n \pi)}{n \sqrt{\pi}} \Big) \\
    &= \frac{-2 \cos(n \pi)}{n \sqrt{\pi}} + \frac{1}{n \sqrt{\pi}} + \frac{1}{n \sqrt{\pi}} \\
    &= \frac{-2 \cos(n \pi)}{n \sqrt{\pi}} + \frac{2}{n \sqrt{\pi}}
  \end{split}
\ee
We cannot say that $\cos(n \pi)$ will always be equal to 1 because it can alternate in sign.
But, we since we know that it will either be 1 or $-1$ depending on the value of n, we can replace $\cos(n \pi)$ with $(-1)^n$, giving
\be
  \begin{split}
    B_n = \frac{-2 (-1)^n}{n \sqrt{\pi}} + \frac{2}{n \sqrt{\pi}} = \frac{2 (-1)^{n+1}}{n \sqrt{\pi}} + \frac{2}{n \sqrt{\pi}} = \frac{2}{n \sqrt{\pi}} ((-1)^{n+1} + 1)
  \end{split}
\ee
And this is fine as an answer as long as it is specified that $n = 1, 2, 3, \hdots$.
If you wanted to make this answer cleaner, you can say that
\be
  B_n =
  \left\{
    \begin{array}{ll}
      0 & \quad \text{for all even n} \\
      \frac{4}{n \sqrt{\pi}} & \quad \text{for all odd n}
    \end{array}
  \right.
\ee
So our Fourier series becomes
\be
  \begin{split}
    \ket{f(\theta)} &= \sum_{n=0}^{\infty} B_n \ket{s_n} \\
    &=  \sum_{n=1}^{\infty} \Big( \frac{4}{n \sqrt{\pi}} \Big) \frac{1}{\sqrt{\pi}} \sin(n \theta) \\
    f(\theta) &= \frac{4}{\pi} \sum_{n=1}^{\infty} \frac{\sin(n \theta)}{n} \\
    &= \frac{4}{\pi} \Big( \sin(\theta) + \frac{\sin(3 \theta)}{3} + \frac{\sin(5 \theta)}{5} + \hdots \Big)
  \end{split}
\ee
where $n = 1, 3, 5, \hdots$.

If we plot truncated versions of our Fourier series on top of our original function
%==============================================================================%
%  Need a plot of n = 5 and a plot of n = 15 next to each other right here.
%==============================================================================%
we can see that if we add more terms from the Fourier series, the series becomes a better and better representation of the original function.
This kind of behavior of the Fourier series representations of piecewise functions such as this one is known as Gibb's Phenomenon or Gibb's oscillations.

Consider another example
\be
  f(\theta) = \cos(k \theta) \qquad (- \pi < \theta < \pi)
\ee
where k is a real number but it is not restricted to integers.
Just by looking at the equation, we can immediately tell that we only need cosine terms, so we will only compute the $A_n$'s for $n = 0, 1, 2, \hdots$
\be
  \begin{split}
    A_n &= \braket{c_n|f(\theta)} \\
    &= \frac{1}{\sqrt{\pi}} \int_{-\pi}^{\pi} \cos(n \theta) f(\theta) d\theta \\
    &= \frac{1}{\sqrt{\pi}} \int_{-\pi}^{\pi} \cos(n \theta) \cos(k \theta) d\theta
  \end{split}
\ee
We can exploit the properties of even functions, since an even function times an even function is another even function, and write our integral as
\be
  \frac{1}{\sqrt{\pi}} \int_{-\pi}^{\pi} \cos(n \theta) \cos(k \theta) d\theta
  = \frac{2}{\sqrt{\pi}} \int_{0}^{\pi} \cos(n \theta) \cos(k \theta) d\theta
\ee
And we can substitute a product identity $\cos(x)\cos(y) = \frac{1}{2}[\cos(x - y) + \cos(x + y)]$ to make the integral easier to compute
\be
  \begin{split}
    \frac{2}{\sqrt{\pi}} \int_{0}^{\pi} \cos(n \theta) \cos(k \theta) d\theta &= \frac{2}{\sqrt{\pi}} \int_{0}^{\pi} \frac{1}{2} [\cos(n \theta - k \theta) + \cos(n \theta + k \theta) ] d \theta \\
    &= \frac{1}{\sqrt{\pi}} \int_{0}^{\pi} \cos[(n - k) \theta] d \theta + \frac{1}{\sqrt{\pi}} \int_{0}^{\pi} \cos[(n + k) \theta] d \theta \\
    &= \frac{1}{\sqrt{\pi}} \Big( \frac{\sin[(n - k) \theta]}{(n - k)} \Big) \Big|_0^{\pi} + \frac{1}{\sqrt{\pi}} \Big( \frac{\sin[(n + k) \theta]}{(n + k)} \Big) \Big|_0^{\pi} \\
    &= \frac{1}{\sqrt{\pi}} \frac{\sin[(n - k) \pi]}{(n-k)} + \frac{1}{\sqrt{\pi}} \frac{\sin[(n + k) \pi]}{(n+k)} \\
    &= \frac{1}{\sqrt{\pi}} \frac{(n+k)}{(n+k)} \frac{\sin[(n - k) \pi]}{(n-k)} + \frac{1}{\sqrt{\pi}} \frac{(n-k)}{(n-k)} \frac{\sin[(n + k) \pi]}{(n+k)} \\
    &= \frac{(n+k)}{\sqrt{\pi}} \frac{\sin[(n - k) \pi]}{(n^2-k^2)} + \frac{(n-k)}{\sqrt{\pi}} \frac{\sin[(n + k) \pi]}{(n^2-k^2)} \\
    &= \frac{(n+k)\sin[n \pi - k \pi] + (n-k) \sin[n \pi + k \pi]}{\sqrt{\pi} (n^2 - k^2)} \\
    &= \frac{1}{\sqrt{\pi}(n^2 - k^2)} [(n+k)\sin(n \pi - k \pi) + (n-k) \sin(n \pi + k \pi) ]
  \end{split}
\ee
Using two more trig identities $$\sin(x - y) = \sin(x) \cos(y) - \cos(x) \sin(y) \quad \text{and} \quad \sin(x + y) = \sin(x) \cos(y) + \cos(x) \sin(y)$$
we can make another substitution to get
\be
  \begin{split}
    \frac{2}{\sqrt{\pi}} \int_{0}^{\pi} \cos(n \theta) \cos(k \theta) d\theta &= \frac{1}{(n^2 - k^2)} [(n+k)\sin(n \pi - k \pi) + (n-k) \sin(n \pi + k \pi) ] \\
    &= \frac{1}{\sqrt{\pi}(n^2 - k^2)} \Big[ (n+k)[\sin(n \pi)\cos(k \pi) - \cos(n \pi) \sin(k \pi)] + \\
    & \qquad (n - k)[\sin(n \pi)\cos(k \pi) + \cos(n \pi) \sin(k \pi)] \Big]
  \end{split}
\ee
And we can set the two $\sin(n \pi)$'s to zero because n is an integer and we get
\be
  \begin{split}
    \frac{2}{\sqrt{\pi}} \int_{0}^{\pi} \cos(n \theta) \cos(k \theta) d\theta &= \frac{1}{\sqrt{\pi}(n^2 - k^2)} \Big[-(n+k)\cos(n \pi) \sin(k \pi) + (n-k) \cos(n \pi) \sin(k \pi) \Big] \\
    &= \frac{\cos(n \pi) \sin(k \pi)}{\sqrt{\pi}(n^2 - k^2)}[(n-k)-(n+k)] \\
    &= \frac{-2k \cos(n \pi) \sin(k \pi)}{\sqrt{\pi}(n^2 - k^2)} \\
    &= \frac{2k \cos(n \pi) \sin(k \pi)}{\sqrt{\pi}(k^2 - n^2)}
  \end{split}
\ee
which is valid for $n = 0, 1, 2, \hdots$.
If we want to make the result look cleaner, we can do what we did before with $\cos(n \pi)$ and replace it with $(-1)^n$, which gives
\be
  A_n = \frac{2}{\sqrt{\pi}} \int_{0}^{\pi} \cos(n \theta) \cos(k \theta) d\theta = \frac{(-1)^n 2k \sin(k \pi)}{\sqrt{\pi}(k^2 - n^2)}
\ee
So our Fourier series becomes
\be
  \begin{split}
    \ket{f(\theta)} &= \sum_{n=0}^{\infty} A_n \ket{c_n} \\
    &= \sum_{n=0}^{\infty} \Big( \frac{(-1)^n 2k \sin(k \pi)}{\sqrt{\pi}(k^2 - n^2)} \Big) \frac{1}{\sqrt{\pi}} \cos(n \theta) \\
    \cos(k \theta) &= \sum_{n=0}^{\infty} \Big( \frac{(-1)^n 2k \sin(k \pi)}{\pi(k^2 - n^2)} \Big) \cos(n \theta) \\
    &= \frac{2k \sin(k \pi)}{\pi} \sum_{n=0}^{\infty} \frac{(-1)^n \cos(n \theta)}{k^2 - n^2} \\
    &= \frac{2k \sin(k \pi)}{\pi} \Big( \frac{1}{k^2} - \frac{\cos(\theta)}{k^2 - 1} + \frac{\cos(2 \theta)}{k^2 - 4} - \hdots \Big)
  \end{split}
\ee

We've talked about functions where $f(\theta)$ is real so our Fourier series can consist of sines and cosines.
But what if $f(\theta)$ was complex?
All we need to do is change our basis from $\sin(n \theta)$ and $\cos(n \theta)$ to $e^{i n \theta}$.
So we can define our series as
\be
  \ket{f(\theta)} = \sum_{n= - \infty}^{\infty} C_n \ket{e_n}
\ee
where the series goes from $- \infty$ to $\infty$ to cover all space.
We can check that our basis vector is orthogonal with itself by checking if
We can normalize our basis by requiring that the projection of the basis vector with itself is equal to one $\braket{e_n|e_n} = 1$, which is the same as integrating over our finite interval $0 \leq \theta \leq 2 \pi$.
\be
  \braket{e_n | e_n} = \int_{0}^{2 \pi} c_n^2 e^{-i n \theta} e^{i n \theta} d \theta = c_n^2 \int_{0}^{2 \pi} 1 d \theta = 2 \pi c_n^2 = 1 \Rightarrow c_n = \frac{1}{\sqrt{2 \pi}}
\ee
So our Fourier series becomes
\be
  \begin{split}
    \ket{f(\theta)} = \sum_{n= - \infty}^{\infty} C_n \ket{e_n} \\
    f(\theta) = \sum_{n= - \infty}^{\infty} C_n \Big( \frac{1}{\sqrt{2 \pi}} e^{i n \theta} \Big)
  \end{split}
\ee
So we can now find an analytic expression for the weight $C_n$ by projecting the basis vector $\ket{e_n}$ with our function $\ket{f(\theta)}$
\be
  \braket{e_n|f(\theta)} = \sum_{n= - \infty}^{\infty} C_n \braket{e_n|e_n} \quad \Rightarrow \quad C_n = \braket{e_n|f(\theta)} = \frac{1}{\sqrt{2 \pi}} \int_0^{2 \pi} e^{-i n \theta} f(\theta) d \theta
\ee

If we wanted to write a Fourier series for a function that is periodic over a length L, then we substitute $\theta = \frac{2 \pi x}{L}$ and $d\theta = \frac{2 \pi}{L} dx$ to get a new coefficient $C_n$.
\be
  \begin{split}
    C_n = \braket{e_n|f(x)} &= \frac{1}{\sqrt{2 \pi}} \int_0^L e^{- 2 \pi n i / L} f(x) \frac{2 \pi}{L} dx \\
    &= \frac{2 \pi}{L \sqrt{2 \pi}} \int_0^L e^{- 2 \pi n i / L} f(x) dx \\
    &= \frac{\sqrt{2 \pi}}{L} \int_0^L e^{- 2 \pi n i / L} f(x) dx \\
  \end{split}
\ee
Therefore
\be
  \ket{f(x)} = \sum_{n = -\infty}^{\infty} C_n \ket{e_n} \Rightarrow f(x) = \sum_{n = -\infty}^{\infty} C_n \frac{1}{\sqrt{2 \pi}} e^{2 \pi n i / L} \qquad C_n = \braket{e_n|f(x)} \frac{\sqrt{2 \pi}}{L} \int_0^L e^{- 2 \pi n i / L} f(x) dx
\ee



\end{document}
